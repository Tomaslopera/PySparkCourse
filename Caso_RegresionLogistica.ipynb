{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión Logística Titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAVA_HOME fijado a: /Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home\n",
      "which java (kernel): /Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home/bin/java\n",
      "java -version (kernel):\n",
      "openjdk version \"1.8.0_292\"\n",
      "OpenJDK Runtime Environment (AdoptOpenJDK)(build 1.8.0_292-b10)\n",
      "OpenJDK 64-Bit Server VM (AdoptOpenJDK)(build 25.292-b10, mixed mode)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, subprocess\n",
    "\n",
    "java8_home = \"/Library/Java/JavaVirtualMachines/adoptopenjdk-8.jdk/Contents/Home\"\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = java8_home\n",
    "os.environ[\"PATH\"] = os.path.join(java8_home, \"bin\") + os.pathsep + os.environ.get(\"PATH\",\"\")\n",
    "\n",
    "os.environ[\"HADOOP_USER_NAME\"] = os.environ.get(\"USER\", \"tomas\")\n",
    "\n",
    "print(\"JAVA_HOME fijado a:\", os.environ[\"JAVA_HOME\"])\n",
    "try:\n",
    "    print(\"which java (kernel):\", subprocess.check_output([\"which\",\"java\"]).decode().strip())\n",
    "    print(\"java -version (kernel):\")\n",
    "    print(subprocess.check_output([\"java\",\"-version\"], stderr=subprocess.STDOUT).decode())\n",
    "except Exception as e:\n",
    "    print(\"Error llamando a java desde kernel:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/13 21:25:09 WARN Utils: Your hostname, MacBook-Air-de-Tomas-3.local resolves to a loopback address: 127.0.0.1; using 192.168.1.4 instead (on interface en0)\n",
      "25/09/13 21:25:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/09/13 21:25:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/13 21:25:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/09/13 21:25:12 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('myproj').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv('../PySparkCourse/MLData/titanic.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "0",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "2ba0940a-eed1-472f-82b4-7f1529527c29",
       "rows": [
        [
         "0",
         "PassengerId"
        ],
        [
         "1",
         "Survived"
        ],
        [
         "2",
         "Pclass"
        ],
        [
         "3",
         "Name"
        ],
        [
         "4",
         "Sex"
        ],
        [
         "5",
         "Age"
        ],
        [
         "6",
         "SibSp"
        ],
        [
         "7",
         "Parch"
        ],
        [
         "8",
         "Ticket"
        ],
        [
         "9",
         "Fare"
        ],
        [
         "10",
         "Cabin"
        ],
        [
         "11",
         "Embarked"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 12
       }
      },
      "text/plain": [
       "['PassengerId',\n",
       " 'Survived',\n",
       " 'Pclass',\n",
       " 'Name',\n",
       " 'Sex',\n",
       " 'Age',\n",
       " 'SibSp',\n",
       " 'Parch',\n",
       " 'Ticket',\n",
       " 'Fare',\n",
       " 'Cabin',\n",
       " 'Embarked']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected_cols = data.select(['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_data = selected_cols.na.drop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformación de Datos\n",
    "\n",
    "- **VectorAssembler:** Concatena columnas numéricas (Vectores en una sola columna vectorial para features)\n",
    "- **VectorIndexer:** Indexar la categoría (String -> Integer index)\n",
    "- **OneHotEncoder:** Conversión de índice a binarios\n",
    "<br>\n",
    "\n",
    "- **StringIndexer:** Convierte strings a índices numéricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, VectorIndexer, OneHotEncoder, StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gender_indexer = StringIndexer(inputCol='Sex',outputCol='SexIndex')\n",
    "gender_encoder = OneHotEncoder(inputCol='SexIndex',outputCol='SexVec')\n",
    "embark_indexer = StringIndexer(inputCol='Embarked',outputCol='EmbarkIndex')\n",
    "embark_encoder = OneHotEncoder(inputCol='EmbarkIndex',outputCol='EmbarkVec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['Pclass', 'SexVec', 'Age', 'SibSp', 'Parch', 'Fare', 'EmbarkVec'], outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo\n",
    "\n",
    "- **Pipeline:** Encadena transformaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_reg_titanic = LogisticRegression(featuresCol='features',labelCol='Survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[gender_indexer,embark_indexer,\n",
    "                           gender_encoder,embark_encoder,\n",
    "                           assembler,log_reg_titanic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_titanic_data, test_titanic_data = clean_data.randomSplit([0.7,.3])\n",
    "fit_model = pipeline.fit(train_titanic_data)\n",
    "results = fit_model.transform(test_titanic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------+\n",
      "|Survived|            features|prediction|\n",
      "+--------+--------------------+----------+\n",
      "|       0|[1.0,1.0,29.0,1.0...|       0.0|\n",
      "|       0|[1.0,1.0,36.0,0.0...|       1.0|\n",
      "|       0|[1.0,1.0,36.0,1.0...|       0.0|\n",
      "|       0|[1.0,1.0,37.0,0.0...|       1.0|\n",
      "|       0|[1.0,1.0,38.0,0.0...|       0.0|\n",
      "|       0|(8,[0,1,2,6],[1.0...|       0.0|\n",
      "|       0|[1.0,1.0,45.0,0.0...|       0.0|\n",
      "|       0|[1.0,1.0,50.0,1.0...|       0.0|\n",
      "|       0|[1.0,1.0,51.0,0.0...|       0.0|\n",
      "|       0|[1.0,1.0,54.0,0.0...|       0.0|\n",
      "+--------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.select('Survived','features','prediction').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC (ROC): 0.8539886039886044\n",
      "AUC (PR): 0.8312582236915125\n"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator(\n",
    "    rawPredictionCol='rawPrediction', \n",
    "    labelCol='Survived', \n",
    "    metricName='areaUnderROC'\n",
    ")\n",
    "auc_roc = evaluator.evaluate(results)\n",
    "print(\"AUC (ROC):\", auc_roc)\n",
    "\n",
    "# AUC (PR) si quieres precisión-recall area\n",
    "auc_pr = evaluator.setMetricName(\"areaUnderPR\").evaluate(results)\n",
    "print(\"AUC (PR):\", auc_pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Análisis**\n",
    "\n",
    "- **AUC (ROC):** área bajo la curva ROC. Mide la capacidad del modelo para ordenar ejemplos positivos por encima de negativos sin depender de un umbral (1 = perfecto, 0.5 = aleatorio). **Indica que el modelo ordena bien las instancias con un 85% de que un positivo tenga score mayor que un negativo**\n",
    "<br>\n",
    "\n",
    "- **AUC PR:** área bajo la curva Precisión-Recall. Confirma que sobre la clase positiva, el rendimiento en la región de interés también es bueno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = results.filter((col(\"Survived\") == 1.0) & (col(\"prediction\") == 1.0)).count()\n",
    "tn = results.filter((col(\"Survived\") == 0.0) & (col(\"prediction\") == 0.0)).count()\n",
    "fp = results.filter((col(\"Survived\") == 0.0) & (col(\"prediction\") == 1.0)).count()\n",
    "fn = results.filter((col(\"Survived\") == 1.0) & (col(\"prediction\") == 0.0)).count()\n",
    "total = results.count()\n",
    "\n",
    "accuracy = (tp + tn) / total if total > 0 else 0\n",
    "precision_pos = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall_pos = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1_pos = 2 * precision_pos * recall_pos / (precision_pos + recall_pos) if (precision_pos + recall_pos) > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix and Metrics:\n",
      "TP (1,1): 55\n",
      "TN (0,0): 110\n",
      "FP (0,1): 16\n",
      "FN (1,0): 23\n",
      "Total: 204\n",
      "Accuracy: 0.8088\n",
      "Precision (pos): 0.7746\n",
      "Recall (pos): 0.7051\n",
      "F1 (pos): 0.7383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----+\n",
      "|Survived|prediction|count|\n",
      "+--------+----------+-----+\n",
      "|       0|       0.0|  110|\n",
      "|       0|       1.0|   16|\n",
      "|       1|       0.0|   23|\n",
      "|       1|       1.0|   55|\n",
      "+--------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nConfusion Matrix and Metrics:\")\n",
    "print(f\"TP (1,1): {tp}\")\n",
    "print(f\"TN (0,0): {tn}\")\n",
    "print(f\"FP (0,1): {fp}\")\n",
    "print(f\"FN (1,0): {fn}\")\n",
    "print(f\"Total: {total}\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision (pos): {precision_pos:.4f}\")\n",
    "print(f\"Recall (pos): {recall_pos:.4f}\")\n",
    "print(f\"F1 (pos): {f1_pos:.4f}\")\n",
    "\n",
    "results.groupBy(\"Survived\", \"prediction\").count().orderBy(\"Survived\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Análisis**\n",
    "\n",
    "- **Accuracy ≈ 80.9%:** buen nivel global, pero en clasificación binaria la accuracy puede ocultar errores si hay costos diferentes para FP y FN.\n",
    "\n",
    "- **Precision ≈ 77.5%:** cuando el modelo predice “sobrevivió”, ~77.5% de esos casos son correctos — es una tasa razonable.\n",
    "\n",
    "- **Recall ≈ 70.5%:** detecta ~70% de los sobrevivientes reales; se pierden ~29.5% de positivos (FN).\n",
    "\n",
    "- **F1 ≈ 73.8%:** balance decente entre precision y recall.\n",
    "\n",
    "- **FP = 16 (12.7% FPR):** hay falsos positivos, pero relativamente pocos comparado con los negativos totales.\n",
    "\n",
    "- **FN = 23:** se perdieron 23 sobrevivientes reales — si el negocio considera costoso no detectar a un sobreviviente, esto puede ser importante."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
